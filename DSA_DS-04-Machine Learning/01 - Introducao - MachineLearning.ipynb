{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução a Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Introdução\n",
    "<div style=\"text-align:justify;\">\n",
    "\n",
    "**O que é Aprendizado?**<br/>\n",
    "&emsp;&emsp;Aprendizado é a capacidade de se adaptar, modificar e melhorar seu comportamento e suas respostas, sendo portanto uma das propriedades mais importantes dos seres ditos inteligentes, sejam eles humanos ou não. Capacidade de adaptar, corrigir, otimizar, representação e interação com o meio. \n",
    "\n",
    "\n",
    "**O que é Machine Learning?**<br/>\n",
    "&emsp;&emsp;É a técnica usada por cientistas de dados para automatizar a construção de modelos analíticos de dados, usando algoritmos que aprendem de forma iterativa a partir dos dados. Machine learning permite que computadores encontrem insights ocultos sem a necessidade de dizer a eles onde procurar.<br/>\n",
    "&emsp;&emsp;É um subcampo da inteligência artificial que permite dar aos computadores a habilidade de aprender sem que sejam explicitamente programados para isso. Temos vários tipos de aprendizagem, são elas: Supervisionada, não supervisionada, semi supervisionada, aprendizagem por reforço e deep learning.<br/>\n",
    "&emsp;&emsp;Os algoritmos de aprendizagem de máquina, aprendem a induzir uma função ou hipótese capaz de resolver um problema a partir de dados que representam instâncias do problema a ser resolvido.<br/>\n",
    "&emsp;&emsp;O modelo é uma função matemática que tenta prever o futuro.<br/>\n",
    "&emsp;&emsp;O objetivo do aprendizado de máquina é aprender a aproximação da função f que melhor representa a relação entre os atributos de entrada (chamadas variáveis preditoras) com a variável target.<br/>\n",
    "\n",
    "\n",
    "**Inteligência artificial:**<br/>\n",
    "&emsp;&emsp;Conjunto de teorias e de técnicas empregadas com a finalidade de desenvolver máquinas capazes de simular a inteligência humana.\n",
    "\n",
    "**Processo de aprendizagem:**<br/>\n",
    "&emsp;&emsp;Um processo de aprendizagem segue alguns passos, são eles:\n",
    "1. Problema de negócio;\n",
    "2. Datasets;\n",
    "3. Análise exploratória dos dados;\n",
    "4. Preparação dos dados;\n",
    "5. Modelagem;\n",
    "6. Avaliação;\n",
    "7. Deploy (Pegar o modelo e colocar em produção);\n",
    "\n",
    "\n",
    "**Tipos de aprendizagem:**<br/>\n",
    "* **Aprendizagem supervisionada**: São treinados utilizando um conjunto de dados rotulados a respeito das possíveis saídas e o algoritmo tenta encontrar uma função matemática que consiga prever as saídas desejadas com base nos dados de entrada.\n",
    "    * **Classificação**;\n",
    "    * **Regressão**: Um estudo de regressão busca, essencialmente associar uma variável Y (denominada variável resposta ou variável dependente) a uma outra variável X (denominada variável explanatória ou variável independente).<br/>\n",
    "A aprendizagem supervisionada ainda é divida em alguns métodos: <br/>\n",
    "&emsp;&emsp;a. Métodos baseados em instância;<br/>\n",
    "&emsp;&emsp;b. Métodos probabilísticos;\n",
    "* **Aprendizagem não supervisionada**: Ocorre quando o algoritmo aprende sem qualquer resposta associada, deixando a cargo do algoritmo encontrar padrões, vetores, classes, etc. O objetivo é organizar os dados de alguma forma ou descrever sua estrutura.\n",
    "    * **Agrupamento (Clusterização)**: Há diversas categorias de algoritmos de clusterização, são eles: Algoritmos baseados em centroides (k-means, gaussian mixture model, fuzzy c-mean), baseados em conectividade (algoritmos hierárquicos), baseados em densidade (DBSCAN, Optics), probabilísticos (LDA), redução de dimensionalidade (tSNE, PCA, KPCA) e redes neurais/ deep learning (Autoencoders);<br/>\n",
    "São divididos nos seguintes métodos:<br/>\n",
    "&emsp;&emsp;a. Métodos baseados em procura;<br/>\n",
    "&emsp;&emsp;b Métodos baseados em otimização;\n",
    "* **Aprendizagem por indução**: A indução é a forma de inferência lógica que permite que conclusões gerais sejam obtidas de exemplos particulares. Em resumo, é o método por trás dos principais modos de aprendizagem, a supervisionada e a não supervisionada.\n",
    "* **Aprendizagem por reforço**: Aprendizado por interação de acordo com o ambiente na qual está inserido. Qual a melhor ação a tomar de acordo com a recompensa que irá obter. Exemplo do agente tentando aprender a jogar videogame.\n",
    "O aprendizado por reforço é comum em robótica, em que o conjunto de leituras do sensor, em um ponto no tempo, é um ponto de dados e o algoritmo deve escolher a próxima ação do robô.\n",
    "\n",
    "\n",
    "**Treinamento, Validação e Teste**<br/>\n",
    "&emsp;&emsp;Consiste em dividir o conjunto de dados para treinar, validar e testar o modelo. Uma divisão comum é 70% dos dados para treino, 20% para validação e 10% para teste.<br/>\n",
    "&emsp;&emsp;É necessário separar os dados de maneira aleatória.<br/>\n",
    "&emsp;&emsp;**Cross-validation**: Validação cruzada, separar o conjunto de dados em subconjuntos, dessa forma, todos os dados serão testados, validados e treinados.\n",
    "\n",
    "\n",
    "**Definição de Big Data:**<br/>\n",
    "&emsp;&emsp;O big data é definido por 4V’s: Grande Volume de dados com uma Variedade dos dados, Veracidade e Velocidade de obtenção destes dados. Aquilo que é extraído do Big Data gera Valor.\n",
    "\n",
    "\n",
    "**Algoritmos que serão abordados no curso:**<br/>\n",
    "* Estatística e matemática;\n",
    "* Árvores de decisão;\n",
    "* Regressão e Classificação;\n",
    "* Nearest Neighbours - KNN;\n",
    "* K-means;\n",
    "* Redes neurais;\n",
    "* Support Vector Machine - SVM;\n",
    "* Naive Bayes;\n",
    "* Deep Learning;\n",
    "\n",
    "\n",
    "\n",
    "&emsp;&emsp;\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Algoritmos de Machine Learning\n",
    "<div style=\"text-align:justify;\">\n",
    "\n",
    "\n",
    "**Modelos descritivos**: Análise do passado para tomar decisões futuras (BI tradicional).<br/>\n",
    "**Modelos preditivos**: É uma função matemática que, aplicada a uma massa de dados, consegue identificar padrões ocultos e prever o que poderá ocorrer. São os algoritmos de machine learning.<br/>\n",
    "\n",
    "> Nota: Identifique com a maior precisão possível o problema de negócio. Quanto mais precisa a pergunta, mais precisa será a resposta e portanto maior o valor da resposta.\n",
    "\n",
    "**Métodos baseados em instâncias**<br/>\n",
    "Métodos baseados em instâncias assumem que as instâncias podem ser representadas como pontos em um espaço Euclidiano.<br/>\n",
    "Calcula a distância entre os atributos de entrada e todas as classes que o modelo aprendeu.<br/>\n",
    "O principal algoritmo utilizado nesse método é o KNN (K - Nearest Neighbours). Ele classifica xz atributo a ele o rótulo representado mais frequentemente dentre as k amostras mais próximas e utilizando um esquema de votação. Pode ser usado para problemas de classificação e de regressão.\n",
    "Outras medidas de distância:\n",
    "* **Correlação de Pearson**: Coeficiente de correlação usado em estatística. Muito usado em bioinformática.\n",
    "* **Similaridade de Cosseno**: Cosseno do ângulo entre os vetores. Usado para classificação de textos e outros dados de alta dimensão.\n",
    "* **Distância de edição**: Usado para medir distância entre strings. Usado em classificação de textos e bioinformática.\n",
    "\n",
    "**Métodos Probabílisticos Bayeseanos**<br/>\n",
    "Os métodos probabilísticos bayesianos assumem que a probabilidade de um evento A, que pode ser uma classe, dado um evento B, poder o conjunto de valores dos atributos de entrada, não depender apenas da relação entre A e B, mas também da probabilidade de observar A independentemente de B.<br/>\n",
    "Os métodos probabilísticos são relevantes por dois motivos:\n",
    "1. Fornecem algoritmos de aprendizagem práticos:<br/>\n",
    "    * Aprendizagem Naive Bayes;\n",
    "    * Aprendizagem de Redes Bayesianas;\n",
    "Combinam conhecimento a priori com os dados observados.\n",
    "2. Fornece uma estrutura conceitual útil: A “norma de ouro” (regra do menor erro possível) para avaliar outros algoritmos de aprendizagem.\n",
    "O classificador Naive Bayes é provavelmente o classificador mais utilizado em Machine Learning. É baseado na suposição simplificadora de que os valores dos atributos são condicionalmente independentes dado o valor alvo.\n",
    "\n",
    "**Métodos Baseados em Procura**<br/>\n",
    "* **Árvores de decisão**: Usa a estratégia dividir para conquistar. Pode ser usado para problemas de classificação e regressão. Classificam instâncias ordenando as árvores acima (ou abaixo), a partir da raiz até alguma folha. Os algoritmos mais conhecidos são:\n",
    "    * ID3 (Quinlan, 1986): Utiliza a Entropia para descrever a quantidade de desordem associada a um sistema. O algoritmo utiliza a Entropia para encontrar quais atributos causam menos desordem no conjunto de dados. \n",
    "        * A incerteza ou impureza em um nó pode ser medida através da Entropia;\n",
    "        * Se todos os exemplos são da mesma classe, então a entropia assume valor mínimo;\n",
    "        * Se todas as classes têm o mesmo número de exemplos, então a entropia assume o valor máximo.\n",
    "    * C4.5 (Quinlan, 1993): \n",
    "\n",
    "**Métodos Baseados em Otimização**<br/>\n",
    "Os mais comuns são as Redes Neurais Artificiais e SVM (Support Vector Machine).\n",
    "\n",
    "\n",
    "**Clusterização**<br/>\n",
    "O algoritmo consiste em buscar grupos de elementos com a maior similaridade entre cada elemento dentro de uma população. Cada grupos é chamado de “cluster”. As principais medidas de distância são: Distância euclidiana e distância city-block. <br/>\n",
    "Outra técnica bastante utilizada na clusterização é a normalização dos dados, para que as variáveis tenham o mesmo peso. As principais técnicas de normalização são:\n",
    "* Min-max para um atributo f;\n",
    "* Z-score;\n",
    "* Desvio absoluto médio.<br/>\n",
    "Tipos de grupos:\n",
    "* Hard Clustering: Ou o elemento está no grupo ou não está;\n",
    "* Soft Clustering: Cada elemento tem uma probabilidade de estar no grupo.<br/>\n",
    "Tipos de algoritmos de Clustering:\n",
    "* Modelos de conectividade: Modelos baseados em distâncias;\n",
    "* Modelos centroides: Modelos baseados na proximidade dos centróides definidos, por exemplo: K-means.\n",
    "* Modelos de distribuição;\n",
    "* Modelos de densidade;\n",
    "\n",
    "\n",
    "**Métodos Ensemble**<br/>\n",
    "Aprendizado por agrupamento. Selecionam uma coleção de hipóteses e combinam suas previsões. Unimos as saídas de diferentes modelos para encontrar a melhor resposta do problema. Modelos de métodos ensemble:\n",
    "* Bootstrap Aggregating (Bagging): Para construção de múltiplos modelos(normalmente do mesmo tipo) a partir de diferentes subsets no dataset de treino;\n",
    "* Boosting: Para construção de múltiplos modelos (normalmente do mesmo tipo), onde cada modelo aprende a corrigir os erros gerados pelo modelo anterior, dentro da sequência de modelos criados;\n",
    "* Voting: Para construção de múltiplos modelos (normalmente de tipos diferentes) e estatística simples (como a média) são usadas para combinar as previsões.\n",
    "* Adaboost;\n",
    "\n",
    "**Por quê utilizar métodos Ensemble?**\n",
    "* Razões estatística;\n",
    "* Grandes volumes de dados/ Pequenos volume de dados;\n",
    "* Dividir e conquistar;\n",
    "* Seleção do modelo;\n",
    "* Diversidade.\n",
    "\n",
    "**Redução de Dimensionalidade**\n",
    "O aumento no volume de dados não é apenas em volume e de forma vertical, mas ocorre também na horizontal, com o aumento do número de dimensões ou atributos das bases de dados.<br/>\n",
    "O termo dimensionalidade é atribuído ao número de características de uma representação de padrões, ou seja, a dimensão do espaço de características.\n",
    "* **Extração de atributos**: Combinam atributos para formar um novo atributo;\n",
    "* **Seleção de atributos**: Seleciona os atributos seguindo algum critério.<br/>\n",
    "Principais técnicas para redução de dimensionalidade:\n",
    "* **Missing Ratio Value**: Coluna de dados com muitos valores missing podem ser removidas;\n",
    "* **Low variance filter**: Todas as colunas de dados com variância inferior a um determinado limite são removidas;\n",
    "* **High Correlation filter**: Colunas que são muito correlatas entre si podem ser removidas caso sejam superior a um determinado limite. É necessário normalizar os dados.\n",
    "* **Random Forest / Ensemble Tree;**\n",
    "* **Forward Feature Construction**: Começa com um único atributo e vai adicionando atributos a cada iteração do algoritmo.\n",
    "* **Backward feature elimination**: Começa com todos os atributos e a cada iteração vai removendo um atributo por vez.\n",
    "* **Principal Component Analysis (PCA)**: É um algoritmo estatístico que transforma muitas dimensões em poucas dimensões. Os dados precisam estar normalizados.\n",
    "\n",
    "\n",
    "**Modelos Determinísticos x Modelos Estotácasticos**<br/>\n",
    "* **Modelos Determinísticos**: Se a chance de ocorrência é levada em conta e o conceito de probabilidade é introduzido no modelo. Os resultados do modelo são pré-determinados em função dos dados de entrada.\n",
    "* **Modelos Estocásticos**: Possuem uma ou mais variáveis aleatórias, e a saída é aleatória. Os resultados do modelo não dependem somente dos dados de entrada, mas também de outros fatores, normalmente aleatórios. Isso requer um modelo probabilístico.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Como funciona o aprendizado de máquina\n",
    "<div style=\"text-align:justify;\">\n",
    "\n",
    "Um componente chave do processo de aprendizagem é a generalização. Se um algoritmo de Machine Learning não for capaz de generalizar uma função matemática que faça previsões sobre novos conjuntos de dados, ele não está aprendendo nada e sim memorizando os dados, o que é bem diferente.\n",
    "Para poder generalizar os algoritmos se baseiam em 3 componentes:\n",
    "* Representação;\n",
    "* Avaliação;\n",
    "* Otimização.\n",
    "\n",
    "\n",
    "**Função de custo**<br/>\n",
    "A função de custo é uma função que mapeiam o quão bem o algoritmo mapeia a função alvo a partir dos dados fornecidos.\n",
    "\n",
    "\n",
    "**Underfitting x Overfitting**<br/>\n",
    "O underfitting ocorre quando o modelo não consegue aprender sobre os dados, seja por falta de dados no conjunto ou por alguma análise errônea por parte do cientista.\n",
    "O overfitting é quando o modelo aprende tanto sobre os dados que a curva de aprendizado deixa de existir.\n",
    "\n",
    "<img src=\"assets/figura01.png\"/>\n",
    "\n",
    "\n",
    "**Teste de Hipótese**<br/>\n",
    "Formulada uma determinada hipótese particular é necessário coletar dados e com base nestes dados decide-se então sobre a validade ou não da hipótese.\n",
    "Mas o que é exatamente uma hipótese?\n",
    "Uma hipótese estatística é uma afirmação sobre o parâmetro, ou parâmetros, da distribuição de probabilidade de uma característica, X, de uma população.\n",
    "H0: Hipótese Nula: É normalmente formulada com o objetivo de ser rejeitada.\n",
    "H1: Hipótese Alternativa: A rejeição da hipótese nula envolve a aceitação de uma outra hipótese, chamada hipótese alternativa.\n",
    "Quando as hipóteses são formuladas sobre os parâmetros do modelo probabilístico da população, o teste de hipóteses é chamado de Paramétrico. Quando as hipóteses são formuladas sobre outras características do modelo, o teste é chamado de Não paramétrico.\n",
    "\n",
    "\n",
    "**Quando uma hipótese é aceita ou rejeitada?**<br/>\n",
    "Se a diferença entre o que foi observado na amostra e o que era esperado (sob a condição da hipótese verdadeira) não for significativa a hipótese será aceita.\n",
    "Se a diferença entre o que foi observado na amostra e o que era esperado (sob a condição da hipótese verdadeira) for significativa a hipótese será rejeitada.\n",
    "<img src=\"assets/figura02.png\"/>\n",
    "\n",
    "**Valor P**<br/>\n",
    "O valor-p (p-value) é o valor de significância observado. Por exemplo: Dado a hipótese nula, ela será rejeitada se o valor-p for maior que 0.05.\n",
    "De um ponto de vista prático, podemos afirmar que o valor-p representa a chance ou a probabilidade do efeito (ou da diferença) observada ser devido ao acaso e não aos fatores que estão sendo estudados/analisados.\n",
    "\n",
    "\n",
    "**Processo de Aprendizagem**<br/>\n",
    "Para achar a implementação dos pacotes disponíveis em python vá no seguinte diretório:\n",
    "> C:\\Users\\krupc\\anaconda3\\pkgs\\scikit-learn-0.24.2-py39hf11a4ad_1\\Lib\\site-packages\\sklearn<br/>\n",
    "\n",
    "\n",
    "Elementos essenciais do aprendizado de máquina:\n",
    "* Um padrão existe;\n",
    "* Não há um único modelo matemático que explique esse padrão;\n",
    "* Dados disponíveis;<br/>\n",
    "\n",
    "\n",
    "Componentes do processo de aprendizagem: \n",
    "* **Input**: Dados de entrada;\n",
    "* **Output**: Saída, o que resultado que queremos;\n",
    "* **Função alvo**: Fórmula que representa o relacionamento entre o input e o output.\n",
    "* **Dados**: Combinação entre input e output, são os dados históricos.\n",
    "* **Hipótese**: Fórmula a ser usada que buscamos generalizar.<br/>\n",
    "<img src=\"assets/figura03.png\"/>\n",
    "\n",
    "**Modelo de aprendizagem**<br/>\n",
    "O espaço de hipóteses contém os recursos com os quais podemos trabalhar. O algoritmo de aprendizagem recebe os dados e navega pelo espaço de hipóteses a fim de encontrar a melhor hipótese que gera o resultado desejado.\n",
    "Exemplo de modelo de aprendizagem:<br/>\n",
    "<img src=\"assets/figura04.png\"/>\n",
    "\n",
    "**Cost Function**<br/>\n",
    "Descreve quão bem a resposta na área de respostas (espaço de hipóteses) se encaixa no conjunto de dados que está sendo analisado.<br/>\n",
    "Valores menores da Cost Function significam um melhor “fit”. Comparando uma previsão contra o seu valor real, usando uma cost function, determinamos o nível de erro do algoritmo.<br/>\n",
    "Por ser uma formulação matemática, a cost function expressa o nível de erro em uma forma numérica. A cost function transmite o que é realmente importante e significativo para seus propósitos com o algoritmo de aprendizagem.\n",
    "\n",
    "\n",
    "**Gradiente Descendente**<br/>\n",
    "É um algoritmo de otimização utilizado para determinar o valor (os coeficientes) de uma função f que minimize a cost function.<br/>\n",
    "O gradiente descendente tenta encontrar o mínimo global da função. É ideal quando os parâmetros não podem ser calculados analiticamente (por exemplo, usando álgebra linear) e devem ser pesquisados por um algoritmo de otimização.<br/>\n",
    "Implementação do gradiente descendente: “*DSA - Machine Learning/03.08 Gradiente Descendente.mp4*”\n",
    "\n",
    "\n",
    "**Bias (Viés) e Variância**\n",
    "* **Bias**: É a tendência do modelo aprender consistentemente uma generelização incorreta.\n",
    "* **Variância**: É a tendência de se aprender fatos aleatórios independentemente do sinal real.\n",
    "* Os modelos mais simples têm viés alto mas variância baixa (underfitting). Os modelos mais complexos têm viés baixo mas variância alta (overfitting).\n",
    "<img src=\"assets/figura05.png\"/>\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
